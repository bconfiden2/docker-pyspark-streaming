{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "779541df",
   "metadata": {},
   "source": [
    "# Programming Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976af9c0",
   "metadata": {},
   "source": [
    "Structured Streaming 은 SparkSQL 위에서 동작하며, sql 의 최적화 기법이나 falut-tolerance, scalability 등을 모두 상속받음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6437c84",
   "metadata": {},
   "source": [
    "Structured Streaming 의 핵심 아이디어는, 데이터 계속해서 append 되는 테이블(```Unbounded Input Table```)을 통해 데이터 스트림을 처리하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0826e904",
   "metadata": {},
   "source": [
    "![Unbounded Tabel](https://spark.apache.org/docs/3.0.3/img/structured-streaming-stream-as-a-table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb040e40",
   "metadata": {},
   "source": [
    "Unbounded Input Table 위에서 동작하는 쿼리로써, 배치 프로세싱 모델과 똑같은 코드로 스트림 처리를 가능하게 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0807fb2",
   "metadata": {},
   "source": [
    "정적 테이블에 대한 연산으로 표현하고, 이를 내부적으로 스트림 잡으로 변환하여 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39b6291",
   "metadata": {},
   "source": [
    "즉, 배치 데이터를 테이블로 받아오고, 테이블을 조작하거나 집계를 내는 등의 다양한 SQL 쿼리를 통해 데이터를 처리할 수 있다면, 이를 스트림처리에 그대로 가져와서 스트림 데이터가 계속 들어오는 Unbounded Table 에 적용할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb858f3",
   "metadata": {},
   "source": [
    "DataFrame/DataSet api 를 사용하여 테이블에 대해 Aggregation, Window, Join 등이 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2dbbc4",
   "metadata": {},
   "source": [
    "테이블에 행한 쿼리는 결과 테이블(Result Table)을 만들어내는데, 사용자가 지정한 trigger interval 마다 새로운 데이터들이 테이블에 추가되면서 Result Table 을 갱신"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9798e1",
   "metadata": {},
   "source": [
    "![trigger](https://spark.apache.org/docs/3.0.3/img/structured-streaming-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d476c2",
   "metadata": {},
   "source": [
    "소스로부터 데이터를 읽어들이고, 테이블을 처리하여 결과테이블을 업데이트한 뒤 소스를 버림 - 결과테이블 갱신에 필요한 중간 데이터는 최소한으로 보관"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b2e691",
   "metadata": {},
   "source": [
    "많은 다른 스트리밍 시스템들은 사용자가 falut-tolerance 나 data-consistency 를 보증하기 위해서 처리하거나 데이터를 유지해야할 부분이 많지만, Structured Streaming 은 결과테이블 갱신만 책임지면 된다는 점에서 부담감이 적음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbd3af0",
   "metadata": {},
   "source": [
    "## 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95d03a6",
   "metadata": {},
   "source": [
    "1. 데이터가 ```소스```로부터 정의된 스키마에 맞게 들어옴\n",
    "2. 이벤트 스트림이란 ```unbounded table``` 에 추가되는 rows\n",
    "3. 스트림으로부터 결과를 얻기 위해 unbounded table 에서 쿼리를 날림\n",
    "4. 동일한 쿼리를 테이블에 trigger 마다 반복적으로 날려 이벤트의 처리 ```결과 테이블```을 생성\n",
    "5. 결과를 ```싱크```에 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15449dd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920da3d1",
   "metadata": {},
   "source": [
    "## 스파크 세션\n",
    "\n",
    "스파크쉘에서 돌릴 경우는 SparkContext 처럼 SparkSession 이 제공됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72658a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ded9376",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"StructuredStreamingTest\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a59603",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6510bd39",
   "metadata": {},
   "source": [
    "## 소스 (Source)\n",
    "소스란? 스트리밍 데이터 공급자를 나타내는 추상화된 개념 == 단순히 데이터가 생성되는 곳\n",
    "\n",
    "구조적 스트리밍은 외부 스트리밍 소스에서 현재 오프셋을 요청하고, 자기가 이전에 마지막으로 처리한 오프셋과 비교하여 그 사이에 있는 배치 데이터를 가져와 처리.\n",
    "- 신뢰할 수 있는 소스란? 스트리밍 소스를 동일한 순서로 다시 재생시킬 수 있어야 함(스트리밍 프로세스가 실패하더라도, 커밋되지 않은 오프셋을 재생성해서 다시 처리할 수 있어야 함) == ```replayability```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a091f020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value 라는 column 하나, 스트리밍하게 입력받는 데이터들이 각각 row 를 구성하는 DataFrame\n",
    "# Warning 은 무시. 소켓으로부터 스트림을 읽는 것은 서비스 배포용이 아닌 학습용으로만 사용하라는 경고\n",
    "lines = spark.readStream.format(\"socket\").option(\"host\", \"localhost\").option(\"port\", \"5000\").load()\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b54f1f1",
   "metadata": {},
   "source": [
    "### format\n",
    "- file : 파일시스템에 존재하는 파일을 읽어옴, 배치 기반 프로세스를 스트리밍 시스템으로 연결하기 위한 간단한 방법. 스파크 2.3.0 부터 csv, json, parquet, orc, text, textfile 등을 지원\n",
    "- kafka : 카프카 토픽으로부터 데이터를 컨슘해오는 subscriber\n",
    "- **socket : utf-8 로 인코딩된 텍스트 데이터 스트림을 제공하는 tcp 서버에 연결해서 데이터를 받아오는 tcp 클라이언트**\n",
    "- rate : 초당 n개의 레코드를 생성해내게끔 설정해줄 수 있는 스트림 생성기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccfa079",
   "metadata": {},
   "source": [
    "### load\n",
    "\n",
    "load 를 통해 받아온 결과는 스트리밍되는 DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c896f0",
   "metadata": {},
   "source": [
    "|   | value |\n",
    "| --- | --- |\n",
    "| 1 | Hello World |\n",
    "| 2 | Word Count |\n",
    "| 3 | Spark Streaming |\n",
    "| 4 | Streaming World |\n",
    "| 5 | Hello Spark Streaming |\n",
    "| 6 | Streaming with SQL |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e69f3c0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b41449",
   "metadata": {},
   "source": [
    "## SQL operations\n",
    "정적 데이터를 갖고있는 테이블에 다양한 연산을 적용하듯이 프로그래밍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89687010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[word: string]\n"
     ]
    }
   ],
   "source": [
    "# SparkSQL 에서 제공되는 함수를 사용해서 transformation\n",
    "# lines 데이터프레임의 value 컬럼을 공백으로 split 하고, 각 원소들을 row 로 하는 word 컬럼 생성\n",
    "words = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65230e4a",
   "metadata": {},
   "source": [
    "|   | word |\n",
    "| --- | --- |\n",
    "| 1 | Hello |\n",
    "| 2 | World |\n",
    "| 3 | Word |\n",
    "| 4 | Count |\n",
    "| 5 | Spark |\n",
    "| 6 | Streaming |\n",
    "| 7 | Streaming |\n",
    "| 8 | World |\n",
    "| 9 | Hello |\n",
    "| 10 | Spark |\n",
    "| 11 | Streaming |\n",
    "| 12 | Streaming |\n",
    "| 13 | with |\n",
    "| 14 | SQL |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5d21c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[word: string, count: bigint]\n"
     ]
    }
   ],
   "source": [
    "# words 데이터프레임에서 word 로 집계한다음 각 원소들의 수를 센 결과 데이터프레임이 wordCounts\n",
    "# 일반적인 테이블에서 집계하는 것과 동일하지만, 스파크가 지속적으로 소켓에서 데이터를 확인해서, incremental query 를 실행시킴\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "print(wordCounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eae3de",
   "metadata": {},
   "source": [
    "|   | word | count |\n",
    "| --- | --- | --- |\n",
    "| 1 | Hello | 2 |\n",
    "| 2 | World | 2 |\n",
    "| 3 | Word | 1 |\n",
    "| 4 | Count | 1 |\n",
    "| 5 | Spark | 2 |\n",
    "| 6 | Streaming | 4 |\n",
    "| 7 | with | 1 |\n",
    "| 8 | SQL | 1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3354cf30",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5b7c62",
   "metadata": {},
   "source": [
    "## 싱크 (Sink)\n",
    "failure 에 대비하여 멱등하게(idempotent, 스트림 처리를 다시 실행시키더라도 똑같은 결과를 내게) 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1685ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과테이블을 complete 모드로, 콘솔 싱크에 write\n",
    "query = wordCounts.writeStream.outputMode(\"complete\").format(\"console\").start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e539462",
   "metadata": {},
   "source": [
    "### outputMode\n",
    "- append : 이번 인터벌에 들어온 새로운 row 들만 처리, 수신한 스트림을 처리한 결과가 수정되지 않는 경우(proj, filtering, mapping 등의 선형 변환)\n",
    "- complete : 결과 테이블을 싹 갱신해서 싱크에 write. 실질적으로는 낮은 카디널리티 기준에 따라 집계하는 경우에만 권장(row 가 적어야 함)\n",
    "- update : 결과테이블 중 바뀌는 row들만 write (즉, 집계 등과 같이 결과테이블의 다른 row 에 영향을 주는 스트림처리가 아니라면 append 와 동일)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8e8e80",
   "metadata": {},
   "source": [
    "### format\n",
    "- console : stdout 에 출력\n",
    "- memory : 메모리에 테이블을 만들고 지속적인 갱신\n",
    "- file : 파일시스템에 특정 형식으로 - csv, json, avro, text 등\n",
    "- kafka : 카프카 토픽에 프로듀싱"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112bc55e",
   "metadata": {},
   "source": [
    "### start\n",
    "DataStreamWriter(```writeStream```) 가 실제로 수행될 수 있게 전체 Job을 스트리밍 연산으로 구체화시키고 내부적으로 스케줄링 프로세스를 시작\n",
    "\n",
    "쿼리를 관리하는 StreamingQuery 객체 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29aa6d9",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
