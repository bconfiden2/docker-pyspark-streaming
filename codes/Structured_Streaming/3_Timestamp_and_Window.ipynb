{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e126aaaf",
   "metadata": {},
   "source": [
    "# Handling Event-time and Window Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212c448a",
   "metadata": {},
   "source": [
    "이벤트가 생성된 타임라인의 관점에서 처리 로직을 적용하고 싶음 -> 각 기기들에서 timestamp 를 찍어서 데이터를 보냄\n",
    "\n",
    "스트림이 하나의 row 를 이루기 때문에, 타임스탬프는 그 중 하나의 컬럼이 되는 것.\n",
    "\n",
    "처리하는 시스템의 내부 시계가 아닌 생산 시스템의 관점에서 이벤트들의 타임라인을 해석해야 함 - 찍힌 타임스탬프를 기준으로"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2433487",
   "metadata": {},
   "source": [
    "데이터가 들어왔을때, 데이터가 생성된 시간(타임스탬프)가 같이 들어온다고 해봅시다\n",
    "\n",
    "이 때 특정 시간동안 들어오는 데이터에 대해서만 계속 관찰해보고싶음 예를 들어 실시간 검색어\n",
    "\n",
    "### 슬라이딩 윈도우 그림 하나 필요\n",
    "\n",
    "12시 정각부터 12시 10분까지의 키워드 집계 -> 12시 10분의 실검 => 10분의 윈도우\n",
    "\n",
    "12시 1분부터 12시 11분까지의 키워드 집계 -> 12시 11분의 실검 => 10분의 윈도우\n",
    "\n",
    "...\n",
    "\n",
    "12시 9분부터 12시 19분까지의 키워드 집계 -> 12시 19분의 실검 => 10분의 윈도우\n",
    "\n",
    "==> 보고기간(슬라이딩) 1분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dbb1fc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82f4168",
   "metadata": {},
   "source": [
    "### 워터마크"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106b4619",
   "metadata": {},
   "source": [
    "일반적으로 타임스탬프로 선언된 필드가 단조 증가하면서 타임라인이 증가 -> 이벤트가 늦게 도착할 수 있음\n",
    "\n",
    "현재 설정된 타임라인보다 일정 시간 이상 차이나는 이벤트들을 폐기시키는 워터마크\n",
    "\n",
    "지금 열심히 9분부터 19분까지 키워드 집계 내고 있는데, 갑자기 12시 2분에 만들어진 데이터가 뭐 네트워크 문제 떄문에 이제 도착했음\n",
    "\n",
    "물론 구조적스트리밍은 내부적으로 이를 반영 가능 - 집계 낸 중간데이터를 일정 기간 좀 유지하고 있어서, 늦게 온 애들도 집계내서 갱신되게\n",
    "\n",
    "근데 이게 무한정 할 수 있는게 아니니까, 저 \"일정기간\"을 어떻게 잡을건지 bound 를 쳐놔야함\n",
    "\n",
    "이 경계선은 그니까, 얼마나 데이터가 늦게 도착해야 집계에서 빼버릴 것인지를 결정하는 친구 - 이를 워터마킹이라고 함\n",
    "\n",
    "### 워터마크 그림 필요\n",
    "\n",
    "스파크엔진이 현재 이벤트타임을 추적해서 너무 오래된 친구들은 싸그리싹싹\n",
    "\n",
    "엔진에 계속해서 데이터가 들어오는데, 앵간치 타임스탬프가 계속 증가해나갈것이고, 이 타임스탬프 최댓값을 엔진이 계속 추적(윈도우 보고시점에만 확인)\n",
    "\n",
    "이 추적값에서 (워터마크 크기) 안에 들어오는 타임스탬프 데이터들만 해당 윈도우 집계에 반영시킴\n",
    "\n",
    "그니까 이제 애초에 워터마크를 벗어난 윈도우친구들은 더이상 수정안된다는 뜻, 워터마크 바깥의 데이터가 늦게 들어오면 그냥 드랍해버림"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e18032",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dbecab",
   "metadata": {},
   "source": [
    "## 테이블 처리 로직 짜기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61734858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|2021-01-01 00:00:...|\n",
      "|2021-01-01 00:00:...|\n",
      "|2021-01-01 00:00:...|\n",
      "|2021-01-01 00:00:...|\n",
      "|2021-01-01 00:00:...|\n",
      "|2021-01-01 00:00:...|\n",
      "|2021-01-01 00:00:...|\n",
      "|2021-01-01 00:00:...|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "spark=SparkSession.builder.appName(\"sparkdf\").getOrCreate()\n",
    "data = [(\"2021-01-01 00:00:01, A B % ^ & *\"),\n",
    "        (\"2021-01-01 00:00:02, E 1@ a#$% B*()_+\"),\n",
    "        (\"2021-01-01 00:00:03, a b c d\"),\n",
    "        (\"2021-01-01 00:00:03, a d e d e\"),\n",
    "        (\"2021-01-01 00:00:04, f f a\"),\n",
    "        (\"2021-01-01 00:00:06, b c d %\"),\n",
    "        (\"2021-01-01 00:00:09, a \"),\n",
    "        (\"2021-01-01 00:00:09,  a a a a! !a !a! !@#A@#$%^&*\")]\n",
    "\n",
    "lines = spark.createDataFrame(data, StringType())\n",
    "lines.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5985453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|          timestamp|            sentence|\n",
      "+-------------------+--------------------+\n",
      "|2021-01-01 00:00:01|         A B % ^ & *|\n",
      "|2021-01-01 00:00:02|    E 1@ a#$% B*()_+|\n",
      "|2021-01-01 00:00:03|             a b c d|\n",
      "|2021-01-01 00:00:03|           a d e d e|\n",
      "|2021-01-01 00:00:04|               f f a|\n",
      "|2021-01-01 00:00:06|             b c d %|\n",
      "|2021-01-01 00:00:09|                  a |\n",
      "|2021-01-01 00:00:09|  a a a a! !a !a!...|\n",
      "+-------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp, split\n",
    "\n",
    "tmp = split(lines.value, \",\")\n",
    "lines = lines.withColumn(\"timestamp\", to_timestamp(tmp[0])).withColumn(\"sentence\", tmp[1]).select(\"timestamp\", \"sentence\")\n",
    "lines.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7987aa1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+\n",
      "|          timestamp|word|\n",
      "+-------------------+----+\n",
      "|2021-01-01 00:00:01|   a|\n",
      "|2021-01-01 00:00:01|   b|\n",
      "|2021-01-01 00:00:02|   e|\n",
      "|2021-01-01 00:00:02|   1|\n",
      "|2021-01-01 00:00:02|   a|\n",
      "|2021-01-01 00:00:02|   b|\n",
      "|2021-01-01 00:00:03|   a|\n",
      "|2021-01-01 00:00:03|   b|\n",
      "|2021-01-01 00:00:03|   c|\n",
      "|2021-01-01 00:00:03|   d|\n",
      "|2021-01-01 00:00:03|   a|\n",
      "|2021-01-01 00:00:03|   d|\n",
      "|2021-01-01 00:00:03|   e|\n",
      "|2021-01-01 00:00:03|   d|\n",
      "|2021-01-01 00:00:03|   e|\n",
      "|2021-01-01 00:00:04|   f|\n",
      "|2021-01-01 00:00:04|   f|\n",
      "|2021-01-01 00:00:04|   a|\n",
      "|2021-01-01 00:00:06|   b|\n",
      "|2021-01-01 00:00:06|   c|\n",
      "+-------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, split, lower, regexp_replace, trim\n",
    "\n",
    "words = lines.withColumn(\"word\", explode(split(trim(regexp_replace(lower(lines.sentence), r\"[^a-z0-9 ]\", \"\")), \" \"))).select(\"timestamp\", \"word\")\n",
    "words.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a17b4d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b6cddf",
   "metadata": {},
   "source": [
    "## 배치 쿼리를 스트림처리에 동일하게 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc467cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 08:43:14 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"StructuredStreamingTest\").getOrCreate()\n",
    "lines = spark.readStream.format(\"socket\").option(\"host\", \"localhost\").option(\"port\", \"5000\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89022af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = split(lines.value, \",\")\n",
    "lines = lines.withColumn(\"timestamp\", to_timestamp(tmp[0])).withColumn(\"sentence\", tmp[1]).select(\"timestamp\", \"sentence\")\n",
    "words = lines.withColumn(\"word\", explode(split(trim(regexp_replace(lower(lines.sentence), r\"[^a-z0-9 ]\", \"\")), \" \"))).select(\"timestamp\", \"word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dac34fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n",
    "\n",
    "wwc = words.withWatermark(\"timestamp\", \"15 minutes\")\\\n",
    "           .groupBy(window(words.timestamp, \"10 minutes\", \"1 minutes\"), words.word)\\\n",
    "           .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d89e02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 08:43:15 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f838cf5c-b341-4192-95b4-44936765fc31. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "21/09/29 08:43:15 WARN TextSocketMicroBatchStream: Stream closed by localhost:5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+----+\n",
      "|timestamp|word|\n",
      "+---------+----+\n",
      "+---------+----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+---------+-------------------+\n",
      "|timestamp|               word|\n",
      "+---------+-------------------+\n",
      "|     null|1443096810189434880|\n",
      "+---------+-------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2166/62554521.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"console\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark-3.0.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "query = words.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b428be58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b496cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d99bda0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
