{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e126aaaf",
   "metadata": {},
   "source": [
    "# Handling Event-time and Window Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212c448a",
   "metadata": {},
   "source": [
    "이벤트가 생성된 타임라인의 관점에서 처리 로직을 적용하고 싶음 -> 각 기기들에서 timestamp 를 찍어서 데이터를 보냄\n",
    "\n",
    "스트림이 하나의 row 를 이루기 때문에, 타임스탬프는 그 중 하나의 컬럼이 되는 것.\n",
    "\n",
    "처리하는 시스템의 내부 시계가 아닌 생산 시스템의 관점에서 이벤트들의 타임라인을 해석해야 함 - 찍힌 타임스탬프를 기준으로"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2433487",
   "metadata": {},
   "source": [
    "데이터가 들어왔을때, 데이터가 생성된 시간(타임스탬프)가 같이 들어온다고 해봅시다\n",
    "\n",
    "이 때 특정 시간동안 들어오는 데이터에 대해서만 계속 관찰해보고싶음 예를 들어 실시간 검색어\n",
    "\n",
    "### 슬라이딩 윈도우 그림 하나 필요\n",
    "\n",
    "12시 정각부터 12시 10분까지의 키워드 집계 -> 12시 10분의 실검 => 10분의 윈도우\n",
    "\n",
    "12시 1분부터 12시 11분까지의 키워드 집계 -> 12시 11분의 실검 => 10분의 윈도우\n",
    "\n",
    "...\n",
    "\n",
    "12시 9분부터 12시 19분까지의 키워드 집계 -> 12시 19분의 실검 => 10분의 윈도우\n",
    "\n",
    "==> 보고기간(슬라이딩) 1분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dbb1fc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82f4168",
   "metadata": {},
   "source": [
    "### 워터마크"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106b4619",
   "metadata": {},
   "source": [
    "일반적으로 타임스탬프로 선언된 필드가 단조 증가하면서 타임라인이 증가 -> 이벤트가 늦게 도착할 수 있음\n",
    "\n",
    "현재 설정된 타임라인보다 일정 시간 이상 차이나는 이벤트들을 폐기시키는 워터마크\n",
    "\n",
    "지금 열심히 9분부터 19분까지 키워드 집계 내고 있는데, 갑자기 12시 2분에 만들어진 데이터가 뭐 네트워크 문제 떄문에 이제 도착했음\n",
    "\n",
    "물론 구조적스트리밍은 내부적으로 이를 반영 가능 - 집계 낸 중간데이터를 일정 기간 좀 유지하고 있어서, 늦게 온 애들도 집계내서 갱신되게\n",
    "\n",
    "근데 이게 무한정 할 수 있는게 아니니까, 저 \"일정기간\"을 어떻게 잡을건지 bound 를 쳐놔야함\n",
    "\n",
    "이 경계선은 그니까, 얼마나 데이터가 늦게 도착해야 집계에서 빼버릴 것인지를 결정하는 친구 - 이를 워터마킹이라고 함\n",
    "\n",
    "### 워터마크 그림 필요\n",
    "\n",
    "스파크엔진이 현재 이벤트타임을 추적해서 너무 오래된 친구들은 싸그리싹싹\n",
    "\n",
    "엔진에 계속해서 데이터가 들어오는데, 앵간치 타임스탬프가 계속 증가해나갈것이고, 이 타임스탬프 최댓값을 엔진이 계속 추적(윈도우 보고시점에만 확인)\n",
    "\n",
    "이 추적값에서 (워터마크 크기) 안에 들어오는 타임스탬프 데이터들만 해당 윈도우 집계에 반영시킴\n",
    "\n",
    "그니까 이제 애초에 워터마크를 벗어난 윈도우친구들은 더이상 수정안된다는 뜻, 워터마크 바깥의 데이터가 늦게 들어오면 그냥 드랍해버림"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e18032",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dbecab",
   "metadata": {},
   "source": [
    "## 테이블 처리 로직 짜기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61734858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|2021-01-01 00:00:...|\n",
      "|2021-01-01 00:00:...|\n",
      "|2021-01-01 00:00:...|\n",
      "|2021-01-01 00:00:...|\n",
      "|2021-01-01 00:00:...|\n",
      "|2021-01-01 00:00:...|\n",
      "|2021-01-01 00:00:...|\n",
      "|2021-01-01 00:00:...|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "spark=SparkSession.builder.appName(\"sparkdf\").getOrCreate()\n",
    "data = [(\"2021-01-01 00:00:01, A B % ^ & *\"),\n",
    "        (\"2021-01-01 00:00:02, E 1@ a#$% B*()_+\"),\n",
    "        (\"2021-01-01 00:00:03, a b c d\"),\n",
    "        (\"2021-01-01 00:00:03, a d e d e\"),\n",
    "        (\"2021-01-01 00:00:04, f f a\"),\n",
    "        (\"2021-01-01 00:00:06, b c d %\"),\n",
    "        (\"2021-01-01 00:00:09, a \"),\n",
    "        (\"2021-01-01 00:00:09,  a a a a! !a !a! !@#A@#$%^&*\")]\n",
    "\n",
    "lines = spark.createDataFrame(data, StringType())\n",
    "lines.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5985453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|          timestamp|            sentence|\n",
      "+-------------------+--------------------+\n",
      "|2021-01-01 00:00:01|         A B % ^ & *|\n",
      "|2021-01-01 00:00:02|    E 1@ a#$% B*()_+|\n",
      "|2021-01-01 00:00:03|             a b c d|\n",
      "|2021-01-01 00:00:03|           a d e d e|\n",
      "|2021-01-01 00:00:04|               f f a|\n",
      "|2021-01-01 00:00:06|             b c d %|\n",
      "|2021-01-01 00:00:09|                  a |\n",
      "|2021-01-01 00:00:09|  a a a a! !a !a!...|\n",
      "+-------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp, split\n",
    "\n",
    "tmp = split(lines.value, \",\")\n",
    "lines = lines.withColumn(\"timestamp\", to_timestamp(tmp[0])).withColumn(\"sentence\", tmp[1]).select(\"timestamp\", \"sentence\")\n",
    "lines.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7987aa1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+\n",
      "|          timestamp|word|\n",
      "+-------------------+----+\n",
      "|2021-01-01 00:00:01|   a|\n",
      "|2021-01-01 00:00:01|   b|\n",
      "|2021-01-01 00:00:02|   e|\n",
      "|2021-01-01 00:00:02|   1|\n",
      "|2021-01-01 00:00:02|   a|\n",
      "|2021-01-01 00:00:02|   b|\n",
      "|2021-01-01 00:00:03|   a|\n",
      "|2021-01-01 00:00:03|   b|\n",
      "|2021-01-01 00:00:03|   c|\n",
      "|2021-01-01 00:00:03|   d|\n",
      "|2021-01-01 00:00:03|   a|\n",
      "|2021-01-01 00:00:03|   d|\n",
      "|2021-01-01 00:00:03|   e|\n",
      "|2021-01-01 00:00:03|   d|\n",
      "|2021-01-01 00:00:03|   e|\n",
      "|2021-01-01 00:00:04|   f|\n",
      "|2021-01-01 00:00:04|   f|\n",
      "|2021-01-01 00:00:04|   a|\n",
      "|2021-01-01 00:00:06|   b|\n",
      "|2021-01-01 00:00:06|   c|\n",
      "+-------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, split, lower, regexp_replace, trim\n",
    "\n",
    "words = lines.withColumn(\"word\", explode(split(trim(regexp_replace(lower(lines.sentence), r\"[^a-z0-9 ]\", \"\")), \" \"))).select(\"timestamp\", \"word\")\n",
    "words.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a17b4d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b6cddf",
   "metadata": {},
   "source": [
    "## 배치 쿼리를 스트림처리에 동일하게 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc467cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 00:31:55 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"StructuredStreamingTest\").getOrCreate()\n",
    "lines = spark.readStream.format(\"socket\").option(\"host\", \"localhost\").option(\"port\", \"5000\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89022af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = split(lines.value, \",\")\n",
    "lines = lines.withColumn(\"timestamp\", to_timestamp(tmp[0])).withColumn(\"sentence\", tmp[1]).select(\"timestamp\", \"sentence\")\n",
    "words = lines.withColumn(\"word\", explode(split(trim(regexp_replace(lower(lines.sentence), r\"[^a-z0-9 ]\", \"\")), \" \"))).select(\"timestamp\", \"word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dac34fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n",
    "\n",
    "wwc = words.withWatermark(\"timestamp\", \"15 minutes\")\\\n",
    "           .groupBy(window(words.timestamp, \"10 minutes\", \"1 minutes\"), words.word)\\\n",
    "           .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d89e02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 00:32:04 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1be68aea-eba5-4044-8864-24056bfc969c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+----+-----+\n",
      "|window|word|count|\n",
      "+------+----+-----+\n",
      "+------+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------------------+----+-----+\n",
      "|              window|word|count|\n",
      "+--------------------+----+-----+\n",
      "|[2020-12-31 23:55...|   a|    1|\n",
      "|[2020-12-31 23:52...|   c|    1|\n",
      "|[2020-12-31 23:52...|   b|    1|\n",
      "|[2020-12-31 23:58...|   a|    1|\n",
      "|[2020-12-31 23:57...|   a|    1|\n",
      "|[2020-12-31 23:53...|   b|    1|\n",
      "|[2020-12-31 23:58...|   1|    1|\n",
      "|[2020-12-31 23:58...|    |    1|\n",
      "|[2020-12-31 23:56...|   1|    1|\n",
      "|[2020-12-31 23:59...|    |    1|\n",
      "|[2020-12-31 23:55...|   c|    1|\n",
      "|[2020-12-31 23:51...|   1|    1|\n",
      "|[2020-12-31 23:55...|   b|    1|\n",
      "|[2020-12-31 23:52...|   a|    1|\n",
      "|[2020-12-31 23:55...|    |    1|\n",
      "|[2020-12-31 23:52...|    |    1|\n",
      "|[2020-12-31 23:54...|   1|    1|\n",
      "|[2020-12-31 23:59...|   b|    1|\n",
      "|[2020-12-31 23:57...|   b|    1|\n",
      "|[2020-12-31 23:53...|   c|    1|\n",
      "+--------------------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+--------------------+----+-----+\n",
      "|              window|word|count|\n",
      "+--------------------+----+-----+\n",
      "|[2020-12-31 23:55...|   a|    1|\n",
      "|[2021-01-01 00:01...|   a|    1|\n",
      "|[2020-12-31 23:52...|   c|    1|\n",
      "|[2020-12-31 23:52...|   b|    1|\n",
      "|[2020-12-31 23:58...|   a|    1|\n",
      "|[2020-12-31 23:57...|   a|    1|\n",
      "|[2020-12-31 23:53...|   b|    1|\n",
      "|[2020-12-31 23:58...|   1|    1|\n",
      "|[2021-01-01 00:10...|   a|    1|\n",
      "|[2020-12-31 23:58...|    |    1|\n",
      "|[2020-12-31 23:56...|   1|    1|\n",
      "|[2020-12-31 23:59...|    |    1|\n",
      "|[2020-12-31 23:55...|   c|    1|\n",
      "|[2020-12-31 23:51...|   1|    1|\n",
      "|[2020-12-31 23:55...|   b|    1|\n",
      "|[2021-01-01 00:08...|   a|    1|\n",
      "|[2020-12-31 23:52...|   a|    1|\n",
      "|[2021-01-01 00:03...|   a|    1|\n",
      "|[2020-12-31 23:55...|    |    1|\n",
      "|[2020-12-31 23:52...|    |    1|\n",
      "+--------------------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+--------------------+----+-----+\n",
      "|              window|word|count|\n",
      "+--------------------+----+-----+\n",
      "|[2020-12-31 23:55...|   a|    1|\n",
      "|[2021-01-01 00:01...|   a|    1|\n",
      "|[2020-12-31 23:52...|   c|    1|\n",
      "|[2020-12-31 23:52...|   b|    1|\n",
      "|[2020-12-31 23:58...|   a|    1|\n",
      "|[2020-12-31 23:57...|   a|    1|\n",
      "|[2020-12-31 23:53...|   b|    1|\n",
      "|[2020-12-31 23:58...|   1|    1|\n",
      "|[2021-01-01 00:13...|   b|    2|\n",
      "|[2021-01-01 00:10...|   a|    1|\n",
      "|[2021-01-01 00:08...|   b|    2|\n",
      "|[2020-12-31 23:58...|    |    1|\n",
      "|[2020-12-31 23:56...|   1|    1|\n",
      "|[2020-12-31 23:59...|    |    1|\n",
      "|[2021-01-01 00:09...|   b|    2|\n",
      "|[2021-01-01 00:11...|   b|    2|\n",
      "|[2020-12-31 23:55...|   c|    1|\n",
      "|[2020-12-31 23:51...|   1|    1|\n",
      "|[2020-12-31 23:55...|   b|    1|\n",
      "|[2021-01-01 00:08...|   a|    1|\n",
      "+--------------------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+--------------------+----+-----+\n",
      "|              window|word|count|\n",
      "+--------------------+----+-----+\n",
      "|[2021-01-01 00:52...|   f|    1|\n",
      "|[2020-12-31 23:55...|   a|    1|\n",
      "|[2021-01-01 00:01...|   a|    1|\n",
      "|[2020-12-31 23:52...|   c|    1|\n",
      "|[2020-12-31 23:52...|   b|    1|\n",
      "|[2021-01-01 00:51...|   f|    1|\n",
      "|[2021-01-01 00:55...|   d|    1|\n",
      "|[2021-01-01 00:55...|   e|    1|\n",
      "|[2021-01-01 00:54...|   f|    1|\n",
      "|[2020-12-31 23:58...|   a|    1|\n",
      "|[2020-12-31 23:57...|   a|    1|\n",
      "|[2020-12-31 23:53...|   b|    1|\n",
      "|[2020-12-31 23:58...|   1|    1|\n",
      "|[2021-01-01 00:13...|   b|    2|\n",
      "|[2021-01-01 00:10...|   a|    1|\n",
      "|[2021-01-01 00:08...|   b|    2|\n",
      "|[2021-01-01 00:51...|   g|    1|\n",
      "|[2020-12-31 23:58...|    |    1|\n",
      "|[2021-01-01 00:55...|   g|    1|\n",
      "|[2021-01-01 00:53...|   f|    1|\n",
      "+--------------------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/29 00:34:46 WARN TextSocketMicroBatchStream: Stream closed by localhost:5000\n"
     ]
    }
   ],
   "source": [
    "query = wwc.writeStream.outputMode(\"complete\").format(\"console\").start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b428be58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc923d21",
   "metadata": {},
   "source": [
    "# Window Operations Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dbc1b3",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5b51c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import window\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.streaming import DataStreamReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8610d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSize = \"60\"\n",
    "slideSize = \"10\"\n",
    "\n",
    "windowDuration = '{} seconds'.format(windowSize)\n",
    "slideDuration = '{} seconds'.format(slideSize)\n",
    "monitoring_dir = 'monitoring_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4d4c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"InteractionCount\")\\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\")\\\n",
    "    .config(\"spark.eventLog.dir\",\"applicationHistory\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcc00f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "userSchema = StructType().add(\"userA\",\"string\")\\\n",
    "                         .add(\"userB\",\"string\")\\\n",
    "                         .add(\"timestamp\",\"timestamp\")\\\n",
    "                         .add(\"interaction\",\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6f8fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitterIDSchema = StructType().add(\"userA\",\"string\")\n",
    "twitterIDs = spark.read.schema(twitterIDSchema).csv('twitterIDs.csv')\n",
    "csvDF = spark\\\n",
    "    .readStream\\\n",
    "    .schema(userSchema)\\\n",
    "    .csv(monitoring_dir)\n",
    "\n",
    "joinedDF = csvDF.join(twitterIDs,\"userA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97b92a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = joinedDF.select(joinedDF['userA'],joinedDF['interaction'],joinedDF['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6ea5eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowedCounts = interactions.groupBy(\n",
    "                       window(interactions.timestamp, windowDuration, slideDuration),interactions.userA)\\\n",
    "                       .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96c17e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = windowedCounts\\\n",
    "    .writeStream\\\n",
    "    .outputMode('complete')\\\n",
    "    .format('console')\\\n",
    "    .option('truncate','false')\\\n",
    "    .option('numRows','10000')\\\n",
    "    .trigger(processingTime='12 seconds')\\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b44df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6370a866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34dacf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "481f4d86",
   "metadata": {},
   "source": [
    "# Window Operations Exercise Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563352e3",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3379174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "# TODO: your path will likely not have 'matthew' in it. Change it to reflect your path.\n",
    "findspark.init('/home/matthew/spark-2.3.0-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5a6964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.streaming import DataStreamWriter, DataStreamReader\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5c2f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"WindowedCount\").master(\"local[*]\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a03072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define a windowDuration and slideDuration\n",
    "windowDuration = '50 seconds'\n",
    "slideDuration = '30 seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d7a4142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set up the `lines` readStream to take in data from a socket stream, AND include the timestamp\n",
    "lines = spark.readStream.format(\"socket\").option(\"host\", \"localhost\")\\\n",
    "         .option(\"port\", 6669).option('includeTimestamp', 'true').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f7093e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the lines into words\n",
    "words = lines.select(explode(split(lines.value, \" \")).alias(\"word\"),lines.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4823917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add a watermark to the data, using the timestamp\n",
    "words = words.withWatermark(\"timestamp\", \"5 seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63e0bd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write out the windowed wordcounts using groupBy(), window(), and count().\n",
    "windowedCounts = words.groupBy(window(words.timestamp, windowDuration, slideDuration),words.word).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26c58fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = windowedCounts.writeStream.outputMode(\"complete\")\\\n",
    "                   .option(\"numRows\", \"100000\")\\\n",
    "                   .option(\"truncate\", \"false\")\\\n",
    "                   .format(\"console\")\\\n",
    "                   .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75132f17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
