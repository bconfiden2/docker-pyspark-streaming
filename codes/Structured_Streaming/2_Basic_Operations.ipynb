{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations on Streaming Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pyspark sql functions](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/functions.html) - SparkSQL 의 함수들을 많이 구현해놨음, 컬럼에 씌울 수 있는 함수들\n",
    "\n",
    "[DataFrame API in scala](https://spark.apache.org/docs/3.0.3/api/scala/org/apache/spark/sql/Dataset.html) - 데이터프레임 객체를 조작하는 함수들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정적 DataFrame 을 조작하는 것과 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split\n",
    "from pyspark.sql.functions import window, current_timestamp\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StructType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7feff03e5e20>\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"StreamingBuyCounter\").getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정의된 스키마에 맞게 소스가 들어온다고 가정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType().add(\"time\", \"string\").add(\"oId\", \"integer\").add(\"cId\", \"integer\")\\\n",
    "              .add(\"qty\", \"integer\").add(\"price\", \"float\").add(\"buy\", \"string\")\n",
    "\n",
    "filestream = spark.readStream.option(\"sep\",\",\").csv(\"/data\", schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| time              | oId   | cId   | qty | price | buy  |\n",
    "|-------------------|-------|-------|-----|-------|------|\n",
    "| 3/18/2018 2:15:18 | 34626 | 39835 |  5  | 4.72  | buy  |\n",
    "| 3/18/2018 2:24:31 | 84260 | 5443  |  9  | 15.26 | buy  |\n",
    "| 3/18/2018 2:33:44 | 56050 | 77178 |  8  | 4.07  | buy  |\n",
    "| 3/18/2018 2:42:57 | 32973 | 34441 |  5  | 15.49 | sell |\n",
    "| 3/18/2018 2:52:10 | 57264 | 98905 |  8  | 1.31  | sell |\n",
    "| 3/18/2018 3:01:23 | 21039 | 5821  |  9  | 18.85 | buy  |\n",
    "| 3/18/2018 3:10:36 | 31880 | 86234 |  6  | 19.22 | buy  |\n",
    "| 3/18/2018 3:19:49 | 82931 | 29797 |  9  | 18.10 | buy  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터프레임 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns: ['time', 'oId', 'cId', 'qty', 'price', 'buy'] \n",
      "\n",
      "schema: StructType(List(StructField(time,StringType,true),StructField(oId,IntegerType,true),StructField(cId,IntegerType,true),StructField(qty,IntegerType,true),StructField(price,FloatType,true),StructField(buy,StringType,true))) \n",
      "\n",
      "dtypes: [('time', 'string'), ('oId', 'int'), ('cId', 'int'), ('qty', 'int'), ('price', 'float'), ('buy', 'string')] \n",
      "\n",
      "== Physical Plan ==\n",
      "StreamingRelation FileSource[/data], [time#0, oId#1, cId#2, qty#3, price#4, buy#5]\n",
      "\n",
      "\n",
      "explain: None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"columns:\", filestream.columns, '\\n')\n",
    "print(\"schema:\", filestream.schema, '\\n')\n",
    "print(\"dtypes:\", filestream.dtypes, '\\n')\n",
    "print(\"explain:\", filestream.explain(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특정 열 선택 (select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qty', 'price', 'buy']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_selection = filestream.select(\"qty\", \"price\", \"buy\")\n",
    "column_selection.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| qty | price | buy  |\n",
    "|-----|-------|------|\n",
    "|  5  | 4.72  | buy  |\n",
    "|  9  | 15.26 | buy  |\n",
    "|  8  | 4.07  | buy  |\n",
    "|  5  | 15.49 | sell |\n",
    "|  8  | 1.31  | sell |\n",
    "|  9  | 18.85 | buy  |\n",
    "|  6  | 19.22 | buy  |\n",
    "|  9  | 18.10 | buy  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 조건 필터링 (where)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('qty', 'int'), ('price', 'float'), ('buy', 'string')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_over_thres = column_selection.where(\"price >= 10\")\n",
    "price_over_thres.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| qty | price | buy  |\n",
    "|-----|-------|------|\n",
    "|  9  | 15.26 | buy  |\n",
    "|  5  | 15.49 | sell |\n",
    "|  9  | 18.85 | buy  |\n",
    "|  6  | 19.22 | buy  |\n",
    "|  9  | 18.10 | buy  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 집계 (groupBy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('buy', 'string'), ('tmp', 'double')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buySellMean = price_over_thres.groupBy(\"buy\").mean(\"price\").withColumnRenamed(\"avg(price)\", \"tmp\")\n",
    "buySellMean.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| buy | tmp |\n",
    "|-----|-------|\n",
    "|  buy  | 17.8575 |\n",
    "|  sell  | 15.49 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 함수적용 (withColumn, udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('buy', 'string'), ('mean_price', 'float')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "func = udf(lambda x: round(x,1), FloatType())\n",
    "result = buySellMean.withColumn(\"mean_price\", func(buySellMean.tmp)).select(\"buy\", \"mean_price\")\n",
    "result.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| buy | mean_price |\n",
    "|-----|-------|\n",
    "|  buy  | 17.9 |\n",
    "|  sell  | 15.5 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lazy eval, sql optimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [buy#5, pythonUDF0#33 AS mean_price#26]\n",
      "+- BatchEvalPython [<lambda>(agg#32)], [pythonUDF0#33]\n",
      "   +- *(4) HashAggregate(keys=[buy#5], functions=[avg(cast(price#4 as double))])\n",
      "      +- StateStoreSave [buy#5], state info [ checkpoint = <unknown>, runId = db50f5aa-f82b-4b21-824f-15d7023727f3, opId = 0, ver = 0, numPartitions = 200], Append, 0, 2\n",
      "         +- *(3) HashAggregate(keys=[buy#5], functions=[merge_avg(cast(price#4 as double))])\n",
      "            +- StateStoreRestore [buy#5], state info [ checkpoint = <unknown>, runId = db50f5aa-f82b-4b21-824f-15d7023727f3, opId = 0, ver = 0, numPartitions = 200], 2\n",
      "               +- *(2) HashAggregate(keys=[buy#5], functions=[merge_avg(cast(price#4 as double))])\n",
      "                  +- Exchange hashpartitioning(buy#5, 200), true, [id=#60]\n",
      "                     +- *(1) HashAggregate(keys=[buy#5], functions=[partial_avg(cast(price#4 as double))])\n",
      "                        +- *(1) Project [price#4, buy#5]\n",
      "                           +- *(1) Filter (isnotnull(price#4) AND (price#4 >= 10.0))\n",
      "                              +- StreamingRelation FileSource[/data], [time#0, oId#1, cId#2, qty#3, price#4, buy#5]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그 외 기타 등등 - 필요에 따라 공식문서에서 찾아 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import reverse, substring, sqrt\n",
    "\n",
    "test = result.withColumn(\"test\", substring(reverse(result.buy), 0,2))\n",
    "test = test.withColumn(\"test2\", sqrt(result.mean_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv 파일 수신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/28 03:45:57 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1e297734-cf7e-490b-bf92-3076940e6e4a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+----------+\n",
      "| buy|mean_price|\n",
      "+----+----------+\n",
      "| buy|      15.4|\n",
      "|sell|      15.5|\n",
      "+----+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----+----------+\n",
      "| buy|mean_price|\n",
      "+----+----------+\n",
      "| buy|      15.4|\n",
      "|sell|      15.5|\n",
      "+----+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+----+----------+\n",
      "| buy|mean_price|\n",
      "+----+----------+\n",
      "| buy|      15.4|\n",
      "|sell|      15.3|\n",
      "+----+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+----+----------+\n",
      "| buy|mean_price|\n",
      "+----+----------+\n",
      "| buy|      15.1|\n",
      "|sell|      15.2|\n",
      "+----+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+----+----------+\n",
      "| buy|mean_price|\n",
      "+----+----------+\n",
      "| buy|      15.1|\n",
      "|sell|      15.3|\n",
      "+----+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+----+----------+\n",
      "| buy|mean_price|\n",
      "+----+----------+\n",
      "| buy|      15.1|\n",
      "|sell|      15.3|\n",
      "+----+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+----+----------+\n",
      "| buy|mean_price|\n",
      "+----+----------+\n",
      "| buy|      15.2|\n",
      "|sell|      15.2|\n",
      "+----+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+----+----------+\n",
      "| buy|mean_price|\n",
      "+----+----------+\n",
      "| buy|      15.1|\n",
      "|sell|      15.2|\n",
      "+----+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+----+----------+\n",
      "| buy|mean_price|\n",
      "+----+----------+\n",
      "| buy|      15.2|\n",
      "|sell|      15.3|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = result.writeStream.format(\"console\").outputMode(\"complete\").start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
